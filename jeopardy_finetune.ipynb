{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zmuhls/jeopardy-lm/blob/main/jeopardy_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbqpCvOwKOeV"
      },
      "source": [
        "# Jeopardy LM: TinyLlama Fine-tuning with LoRA\n",
        "# Google Colab Implementation\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Download Jeopardy questions from a public dataset\n",
        "2. Fine-tune TinyLlama-1.1B using PEFT/LoRA\n",
        "3. Create a simple API server to serve the model\n",
        "4. Evaluate the model's performance on Jeopardy questions\n",
        "\n",
        "This implementation is designed for Google Colab with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "check_gpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595f8b66-3129-435a-f616-9237d1104093"
      },
      "source": [
        "# @title Check for GPU and Colab Environment\n",
        "# @markdown Ensure we're running in Colab with GPU acceleration\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
        "\n",
        "if not IN_COLAB:\n",
        "    print(\"Warning: This notebook is optimized for Google Colab. Some features may not work elsewhere.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "install_dependencies"
      },
      "source": [
        "# @title Install required packages\n",
        "# @markdown Run this cell to install all necessary dependencies\n",
        "\n",
        "!pip install torch==2.0.1 transformers==4.30.2 peft==0.4.0 bitsandbytes==0.40.2 accelerate==0.20.3\n",
        "!pip install datasets==2.13.1 tqdm pandas flask ipywidgets matplotlib tensorboard\n",
        "\n",
        "# Check for GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Training will be slow.\")\n",
        "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > GPU to enable GPU.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mount_drive"
      },
      "source": [
        "# @title Mount Google Drive (Optional)\n",
        "# @markdown Run this to save/load data and models from Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "use_drive = False  # Set to True to use Google Drive for storage\n",
        "\n",
        "if use_drive:\n",
        "    drive.mount('/content/drive')\n",
        "    output_dir = \"/content/drive/MyDrive/jeopardy-lm\"\n",
        "else:\n",
        "    # Create directory for output in Colab local storage\n",
        "    output_dir = \"/content/jeopardy-lm\"\n",
        "\n",
        "import os\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Model and data will be saved to: {output_dir}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_dataset"
      },
      "source": [
        "# @title Download Jeopardy Dataset\n",
        "# @markdown This cell downloads a Jeopardy dataset from Hugging Face Datasets\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Downloading Jeopardy dataset from Hugging Face...\")\n",
        "try:\n",
        "    jeopardy_dataset = load_dataset(\"jeopardy\", split=\"train\")\n",
        "    print(f\"Downloaded {len(jeopardy_dataset)} Jeopardy questions\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Attempting alternative download method...\")\n",
        "    # Alternative: direct download from GitHub\n",
        "    !wget -q https://raw.githubusercontent.com/dw/scratch/master/jeopardy/j.json -O jeopardy.json\n",
        "    import json\n",
        "    with open('jeopardy.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "    jeopardy_dataset = pd.DataFrame(data)\n",
        "    print(f\"Downloaded {len(jeopardy_dataset)} Jeopardy questions from alternative source\")\n",
        "\n",
        "# Convert to DataFrame and save to CSV\n",
        "if not isinstance(jeopardy_dataset, pd.DataFrame):\n",
        "    df = pd.DataFrame(jeopardy_dataset)\n",
        "else:\n",
        "    df = jeopardy_dataset\n",
        "\n",
        "jeopardy_csv_path = os.path.join(output_dir, \"jeopardy_data.csv\")\n",
        "df.to_csv(jeopardy_csv_path, index=False)\n",
        "\n",
        "print(f\"Saved data to {jeopardy_csv_path}\")\n",
        "print(\"Sample data:\")\n",
        "df.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "prepare_dataset"
      },
      "source": [
        "# @title Prepare Jeopardy Dataset\n",
        "# @markdown Convert Jeopardy dataset to instruction tuning format\n",
        "\n",
        "import json\n",
        "\n",
        "def prepare_jeopardy_dataset(jeopardy_df, output_jsonl_path, split_ratio=0.9, max_samples=20000):\n",
        "    \"\"\"\n",
        "    Convert Jeopardy DataFrame to instruction-tuning format\n",
        "    \"\"\"\n",
        "    print(f\"Preparing Jeopardy data for fine-tuning\")\n",
        "\n",
        "    # Format the data as instruction-following examples\n",
        "    formatted_data = []\n",
        "\n",
        "    for _, row in jeopardy_df.iterrows():\n",
        "        category = row['category']\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "\n",
        "        # Format as prompt-completion pair\n",
        "        prompt = f\"Category: {category}\\nClue: {question}\\nAnswer in the form of a question:\"\n",
        "\n",
        "        # Check if answer already has \"what is\" format\n",
        "        answer_lower = answer.lower()\n",
        "        if answer_lower.startswith(\"what is\") or answer_lower.startswith(\"who is\"):\n",
        "            completion = answer\n",
        "        else:\n",
        "            # Decide between \"What is\" and \"Who is\" based on simple heuristics\n",
        "            if any(keyword in answer_lower for keyword in ['person', 'actor', 'actress', 'director', 'author', 'president', 'king', 'queen']):\n",
        "                completion = f\"Who is {answer}?\"\n",
        "            else:\n",
        "                completion = f\"What is {answer}?\"\n",
        "\n",
        "        # Format for instruction tuning\n",
        "        formatted_data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"completion\": completion\n",
        "        })\n",
        "\n",
        "    # Take a subset of the data to speed up training (adjust as needed)\n",
        "    if len(formatted_data) > max_samples:\n",
        "        print(f\"Limiting dataset to {max_samples} samples for faster training\")\n",
        "        formatted_data = formatted_data[:max_samples]\n",
        "\n",
        "    # Split into train and validation\n",
        "    train_size = int(len(formatted_data) * split_ratio)\n",
        "    train_data = formatted_data[:train_size]  # Fixed missing slice index\n",
        "    val_data = formatted_data[train_size:]\n",
        "\n",
        "    # Save to JSONL files\n",
        "    os.makedirs(os.path.dirname(output_jsonl_path), exist_ok=True)\n",
        "\n",
        "    train_path = output_jsonl_path.replace('.jsonl', '_train.jsonl')\n",
        "    val_path = output_jsonl_path.replace('.jsonl', '_val.jsonl')\n",
        "\n",
        "    with open(train_path, 'w') as f:\n",
        "        for item in train_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "    with open(val_path, 'w') as f:\n",
        "        for item in val_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "    print(f\"Saved {len(train_data)} training examples to {train_path}\")\n",
        "    print(f\"Saved {len(val_data)} validation examples to {val_path}\")\n",
        "\n",
        "    return train_path, val_path\n",
        "\n",
        "# Create data directory\n",
        "data_dir = os.path.join(output_dir, \"data\")\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "output_jsonl_path = os.path.join(data_dir, \"jeopardy.jsonl\")\n",
        "\n",
        "# Prepare dataset\n",
        "train_path, val_path = prepare_jeopardy_dataset(df, output_jsonl_path)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "load_dataset"
      },
      "source": [
        "# @title Load Dataset for Fine-tuning\n",
        "# @markdown Load the prepared datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = load_dataset('json', data_files=train_path, split='train')\n",
        "val_dataset = load_dataset('json', data_files=val_path, split='train')\n",
        "\n",
        "print(f\"Loaded {len(train_dataset)} training examples\")\n",
        "print(f\"Loaded {len(val_dataset)} validation examples\")\n",
        "\n",
        "# Show a sample example\n",
        "print(\"\\nSample training example:\")\n",
        "print(f\"Prompt: {train_dataset[0]['prompt']}\")\n",
        "print(f\"Completion: {train_dataset[0]['completion']}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "setup_model"
      },
      "source": [
        "# @title Setup Model for Fine-tuning\n",
        "# @markdown Configure TinyLlama with LoRA adapters\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "# Model parameters\n",
        "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "max_length = 256\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Check if GPU is available, adjust settings accordingly\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Loading model with 8-bit quantization on GPU...\")\n",
        "    load_in_8bit = True\n",
        "    device_map = \"auto\"\n",
        "else:\n",
        "    print(\"GPU not available. Loading smaller model configuration...\")\n",
        "    # If no GPU, use CPU with less memory\n",
        "    load_in_8bit = False\n",
        "    device_map = None\n",
        "\n",
        "# Load base model with appropriate settings\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "        device_map=device_map,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model with 8-bit quantization: {e}\")\n",
        "    print(\"Falling back to standard loading...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        torch_dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "# Prepare for PEFT fine-tuning (only if loaded with quantization)\n",
        "if load_in_8bit:\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,               # Rank of the update matrices\n",
        "    lora_alpha=32,      # Parameter for scaling\n",
        "    lora_dropout=0.05,  # Dropout probability\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    # Target attention modules for TinyLlama\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "# Get PEFT model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters info\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params:,d} || \"\n",
        "        f\"all params: {all_params:,d} || \"\n",
        "        f\"trainable%: {100 * trainable_params / all_params:.2f}%\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "prepare_training_data"
      },
      "source": [
        "# @title Prepare Data for Training\n",
        "# @markdown Tokenize and format data for the trainer\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Define data preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Combine prompt and completion for training\n",
        "    texts = [\n",
        "        f\"{prompt}\\n{completion}\"\n",
        "        for prompt, completion in zip(examples['prompt'], examples['completion'])\n",
        "    ]\n",
        "\n",
        "    # Tokenize\n",
        "    result = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Create labels (for causal LM, labels are the same as input_ids)\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result\n",
        "\n",
        "# Preprocess datasets\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "tokenized_val = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We're not doing masked language modeling\n",
        ")\n",
        "\n",
        "print(f\"Prepared {len(tokenized_train)} training examples\")\n",
        "print(f\"Prepared {len(tokenized_val)} validation examples\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "finetune_model"
      },
      "source": [
        "# @title Fine-tune the Model\n",
        "# @markdown Start the training process\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Training parameters - adjust based on available resources\n",
        "num_epochs = 3\n",
        "\n",
        "# Adjust batch size based on GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    batch_size = 8\n",
        "    gradient_accumulation_steps = 4\n",
        "else:\n",
        "    # Use smaller batches on CPU\n",
        "    batch_size = 2\n",
        "    gradient_accumulation_steps = 8\n",
        "    print(\"WARNING: Training on CPU will be very slow. Consider using a GPU runtime.\")\n",
        "\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    logging_steps=50,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=0.01,\n",
        "    fp16=torch.cuda.is_available(),  # Only use fp16 if GPU is available\n",
        "    bf16=False,\n",
        "    optim=\"adamw_torch\",\n",
        "    warmup_steps=100,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"tensorboard\",  # Enable tensorboard for visualization\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Launch TensorBoard to monitor training (Colab-specific)\n",
        "try:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir {output_dir}\n",
        "except Exception as e:\n",
        "    print(f\"TensorBoard could not be started: {e}\")\n",
        "    print(\"You can still view training progress via the logs.\")\n",
        "\n",
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model_path = f\"{output_dir}/final\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "print(f\"Model fine-tuning complete. Saved to {model_path}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "evaluate_model"
      },
      "source": [
        "# @title Evaluate the Model\n",
        "# @markdown Test the model on Jeopardy questions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from peft import PeftModel\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Create test questions from validation set\n",
        "test_questions = [\n",
        "    {\n",
        "        \"category\": item[\"prompt\"].split(\"\\n\")[0].replace(\"Category: \", \"\"),\n",
        "        \"question\": item[\"prompt\"].split(\"\\n\")[1].replace(\"Clue: \", \"\"),\n",
        "        \"answer\": item[\"completion\"]\n",
        "    }\n",
        "    for item in val_dataset[:100]  # Evaluate on 100 questions\n",
        "]\n",
        "\n",
        "# Load fine-tuned model for evaluation\n",
        "try:\n",
        "    # Load the same model and apply the fine-tuned weights\n",
        "    print(\"Loading base model...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    )\n",
        "\n",
        "    print(f\"Loading fine-tuned adapters from {model_path}...\")\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "    print(\"Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please make sure training was completed successfully.\")\n",
        "    # Exit the evaluation if model loading failed\n",
        "    raise e\n",
        "\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "print(\"Evaluating model on test questions...\")\n",
        "for question in tqdm(test_questions):\n",
        "    category = question[\"category\"]\n",
        "    clue = question[\"question\"]\n",
        "    correct_answer = question[\"answer\"].lower()\n",
        "\n",
        "    # Format the prompt\n",
        "    prompt = f\"Category: {category}\\nClue: {clue}\\nAnswer in the form of a question:\"\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Move inputs to the same device as model\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # Generate with error handling\n",
        "    try:\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode and extract answer\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = response.replace(prompt, \"\").strip().lower()\n",
        "\n",
        "        # Check if answer is correct (simple string matching)\n",
        "        is_correct = correct_answer.lower() in answer or answer in correct_answer.lower()\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        results.append({\n",
        "            \"category\": category,\n",
        "            \"question\": clue,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"model_answer\": answer,\n",
        "            \"is_correct\": is_correct\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer for question '{clue}': {e}\")\n",
        "        continue\n",
        "\n",
        "accuracy = correct / len(test_questions) if test_questions else 0\n",
        "print(f\"Evaluation complete. Accuracy: {accuracy:.2%} ({correct}/{len(test_questions)})\")\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(['Correct', 'Incorrect'], [correct, len(test_questions) - correct])\n",
        "plt.title('Model Evaluation Results')\n",
        "plt.ylabel('Number of Questions')\n",
        "plt.show()\n",
        "\n",
        "# Show some example results\n",
        "print(\"\\nSample results:\")\n",
        "import random\n",
        "sample_results = random.sample(results, min(5, len(results)))\n",
        "for i, result in enumerate(sample_results):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Category: {result['category']}\")\n",
        "    print(f\"Question: {result['question']}\")\n",
        "    print(f\"Correct answer: {result['correct_answer']}\")\n",
        "    print(f\"Model answer: {result['model_answer']}\")\n",
        "    print(f\"Correct: {'✓' if result['is_correct'] else '✗'}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "demo_interface"
      },
      "source": [
        "# @title Create Demo Interface\n",
        "# @markdown Create a simple demo interface in Colab\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Check if we have a model loaded\n",
        "try:\n",
        "    model\n",
        "except NameError:\n",
        "    print(\"ERROR: Model not found. Please run the training and evaluation cells first.\")\n",
        "    model = None\n",
        "\n",
        "if model is not None:\n",
        "    # Create input widgets\n",
        "    category_input = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Enter category',\n",
        "        description='Category:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    clue_input = widgets.Textarea(\n",
        "        value='',\n",
        "        placeholder='Enter clue',\n",
        "        description='Clue:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    submit_button = widgets.Button(\n",
        "        description='Get Answer',\n",
        "        disabled=False,\n",
        "        button_style='primary',\n",
        "        tooltip='Click to get the answer',\n",
        "        icon='check'\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    # Define submit function\n",
        "    def on_submit_clicked(b):\n",
        "        with output_area:\n",
        "            output_area.clear_output()\n",
        "\n",
        "            category = category_input.value\n",
        "            clue = clue_input.value\n",
        "\n",
        "            if not category or not clue:\n",
        "                print(\"Please enter both category and clue.\")\n",
        "                return\n",
        "\n",
        "            print(\"Generating answer...\")\n",
        "\n",
        "            # Format the prompt\n",
        "            prompt = f\"Category: {category}\\nClue: {clue}\\nAnswer in the form of a question:\"\n",
        "\n",
        "            try:\n",
        "                # Tokenize and generate\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "                outputs = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_new_tokens=50,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True\n",
        "                )\n",
        "\n",
        "                # Decode and extract answer\n",
        "                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                answer = response.replace(prompt, \"\").strip()\n",
        "\n",
        "                print(f\"\\nAnswer: {answer}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating answer: {e}\")\n",
        "\n",
        "    # Connect button to function\n",
        "    submit_button.on_click(on_submit_clicked)\n",
        "\n",
        "    # Display widgets\n",
        "    print(\"Jeopardy Question Answering Demo\")\n",
        "    display(category_input)\n",
        "    display(clue_input)\n",
        "    display(submit_button)\n",
        "    display(output_area)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "download_model"
      },
      "source": [
        "# @title Download Fine-tuned Model (Optional)\n",
        "# @markdown Download the fine-tuned model to your local machine\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    # Check if model_path exists\n",
        "    if 'model_path' not in globals() or not os.path.exists(model_path):\n",
        "        print(\"Model not found. Please complete the training process first.\")\n",
        "    else:\n",
        "        # Compress the model directory\n",
        "        print(f\"Compressing model from {model_path}...\")\n",
        "        !zip -r {output_dir}/jeopardy_model.zip {model_path}\n",
        "\n",
        "        # Download the compressed file\n",
        "        print(\"Downloading model zip file...\")\n",
        "        files.download(f\"{output_dir}/jeopardy_model.zip\")\n",
        "\n",
        "        print(\"Model downloaded. To use this model locally:\")\n",
        "        print(\"1. Extract the ZIP file\")\n",
        "        print(\"2. Load it with PeftModel.from_pretrained()\")\n",
        "        print(\"3. Use the provided API server code in the next cell\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading model: {e}\")\n",
        "    print(\"If you're not in Colab, you can manually copy the model files from the output directory.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_server_code"
      },
      "source": [
        "## API Server Code (For Local Use)\n",
        "\n",
        "This code can be used to create a simple API server for the fine-tuned model when running locally. Save this to a file named `api_server.py`:\n",
        "\n",
        "```python\n",
        "# Flask API Server Implementation for Local Use\n",
        "# Save this to a file named \"api_server.py\" on your local machine\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Update these paths to match your local setup\n",
        "base_model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "adapter_path = \"./jeopardy_model\"  # Path to extracted model adapters\n",
        "\n",
        "# Load the tokenizer and model\n",
        "print(\"Loading tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Check for GPU and load model accordingly\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        ")\n",
        "\n",
        "# Load fine-tuned model\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Get JSON data from request\n",
        "    data = request.json\n",
        "    if not data or 'category' not in data or 'clue' not in data:\n",
        "        return jsonify({'error': 'Please provide both category and clue fields'}), 400\n",
        "    \n",
        "    category = data['category']\n",
        "    clue = data['clue']\n",
        "    \n",
        "    # Format the prompt\n",
        "    prompt = f\"Category: {category}\\nClue: {clue}\\nAnswer in the form of a question:\"\n",
        "    \n",
        "    # Generate answer\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if device == \"cuda\":\n",
        "            inputs = inputs.to(\"cuda\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=50,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True\n",
        "            )\n",
        "        \n",
        "        # Decode the output\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = response.replace(prompt, \"\").strip()\n",
        "        \n",
        "        return jsonify({\n",
        "            'category': category,\n",
        "            'clue': clue,\n",
        "            'answer': answer\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/', methods=['GET'])\n",
        "def home():\n",
        "    return \"\"\"\n",
        "    <h1>Jeopardy API Server</h1>\n",
        "    <p>Make a POST request to /predict with JSON data containing 'category' and 'clue'.</p>\n",
        "    <p>Example:</p>\n",
        "    <pre>{\n",
        "  \"category\": \"HISTORY\",\n",
        "  \"clue\": \"This document, signed in 1776, announced that the 13 American colonies were no longer part of Great Britain\"\n",
        "}</pre>\n",
        "    \"\"\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "```\n",
        "\n",
        "### Usage Instructions:\n",
        "\n",
        "1. Install the required packages: `pip install flask torch transformers peft`\n",
        "2. Update the `adapter_path` to point to your extracted model\n",
        "3. Run the server: `python api_server.py`\n",
        "4. Access the API at `http://localhost:5000/predict`\n",
        "\n",
        "### Example cURL request:\n",
        "\n",
        "```bash\n",
        "curl -X POST http://localhost:5000/predict \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"category\":\"SCIENCE\", \"clue\":\"This force keeps planets in orbit around the sun\"}'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_notes"
      },
      "source": [
        "## Final Notes\n",
        "\n",
        "This notebook demonstrates a complete pipeline for fine-tuning TinyLlama-1.1B on Jeopardy questions using LoRA. Here's a summary of what we've accomplished:\n",
        "\n",
        "1. **Data Preparation**: Downloaded and processed Jeopardy questions into an instruction tuning format\n",
        "2. **Model Fine-tuning**: Used PEFT/LoRA to efficiently fine-tune TinyLlama without requiring extensive computational resources\n",
        "3. **Evaluation**: Tested the model's performance on Jeopardy questions\n",
        "4. **Interactive Demo**: Created a simple interface to interact with the model\n",
        "5. **Deployment**: Provided code for deploying the model as an API server\n",
        "\n",
        "### Improving the Model\n",
        "\n",
        "To improve the model's performance, you could try:\n",
        "\n",
        "- Using a larger base model (if you have more computational resources)\n",
        "- Fine-tuning for more epochs\n",
        "- Adjusting hyperparameters (learning rate, batch size, etc.)\n",
        "- Enhancing the data preprocessing (better handling of question format, more data)\n",
        "- Using a more sophisticated evaluation metric\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try creating a more comprehensive interactive application\n",
        "- Experiment with different prompt formats\n",
        "- Apply the same technique to other trivia or question-answering datasets\n",
        "- Deploy the model to a production environment\n",
        "\n",
        "Happy fine-tuning!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}